# How to run DeepSeek-R1 on multiple machines with multiple GPUs using SGLang on Glows.ai

In this tutorial, we will walk you through how to rent GPU cards on Glows.ai and use SGLang to run DeepSeek-R1 models across multiple machines and GPUs.


## Set up Storage Space

After registering and logging in to [Glows.ai](https://glows.ai/), enter [Space Management](https://platform.glows.ai/space) to allocate datadrive (for storage space) and Snapshot (for saving snapshot). If you have already made a purchase, you can skip this step.

Follow the instructions shown in the images: click `Upgrade` on the interface and purchase a 5GB space. During actual use, you can purchase larger storage as needed.

![Space Storage](../../../../tutorials-images/07.DeepSeekSGLang/01.SpaceStorage.png)

After purchasing the storage plan, follow the sequence shown in the image and click `Modify`. In this tutorial, we allocate 3GB to `Snapshot` and 2GB to the `Datadrive` "TW-01", then click `Update`. The space allocation here is shown an example; you can adjust the sizes according to your project needs during actual use.

![Modify Quata](../../../../tutorials-images/07.DeepSeekSGLang/02.ModifyQuota.png)



## Create an instance
On the [Create New](https://platform.glows.ai/create) page, select ` Inference GPU`  -- `NVIDIA GeForce RTX 4090`, and choose the environment `SGLang 0.4.2`. You can also select other machines or environments according to your actual needs.

For this demo, we will use two RTX4090 GPUs. Each RTX4090 has 24GB of VRAM, which is sufficient to run the DeepSeek-R1-Distill-Qwen-14B model. If you want to run the DeepSeek-R1 671B model, based on our tests, you will need either two 8-card H100 machines or a single 8-card H200 machine. If you encounter any issues during the setup process, feel free to contact us at any time.

![CreateNew](../../../../tutorials-images/07.DeepSeekSGLang/03.CreateNew.png)


At the bottom of the page, click `Mount` to mount your own Data Drive to the instance. The benefit of this step is that it allows you to share files across multiple instances, and also makes it easier to permanently store files from the machine or download them to your local device later.
After that, click `Complete Checkout` to finish the rental setup.

![Mount Data Drive](../../../../tutorials-images/07.DeepSeekSGLang/04.MountDataDrive.png)

Since we need to run a large model inference service using SGLang across multiple machines and GPUs, we need to repeat the steps above to create another instance. Once both instances are successfully created, you will see two instance records on the **My Instances** page.

![My Instance](../../../../tutorials-images/07.DeepSeekSGLang/05.MyInstance.png)

It takes about 30–50 seconds for an instance to start. Once its status changes to `Running`, it will be ready for use.

## Launch the program

#### Download the model
After the instance starts successfully, you will see links for SSH access (SSH Port 22), JupyterLab access (HTTP Port 8888), and HTTP Port 8000. Use your preferred remote connection method to access the instance (we recommend using JupyterLab as it is more convenient).

**Note:** If you need to map additional ports during use, you can click the `New Port Forwarding` button to add them.
If the port you are adding uses the HTTP protocol, remember to check the HTTP option when adding the new port.

![Port](../../../../tutorials-images/07.DeepSeekSGLang/06.Port.png)

Click `Open` to access the JupyterLab link on HTTP Port 8888.
Create a new Terminal, then enter the following command to download the model into the /home/DeepSeek-R1-Distill-Qwen-14B directory on the instance. The model is about 28GB in total, and the estimated download time is 5–10 minutes.

**Note:** You need to open JupyterLab on both nodes and run the following command on each to download the model.

```bash
huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --local-dir /home/DeepSeek-R1-Distill-Qwen-14B
```

![Download](../../../../tutorials-images/07.DeepSeekSGLang/07.Download.png)

#### Create a cluster
While the model is downloading, we can start setting up internal networking between the two instances using Glows.ai **Mesh Clustering** feature.


Go to **Mesh Clustering** page and follow the steps shown in the guide to create a new Network Group.
Important settings:

- Mesh Type：Network Group
- Region：TW-01
- Name：You can choose any name you like, for example, SGLang.

![Create Network Group](../../../../tutorials-images/07.DeepSeekSGLang/08.CreateNetworkGroup.png)

Click `Add Instance` to add the two instances you created earlier to the Network Group.

![Add Instance](../../../../tutorials-images/07.DeepSeekSGLang/09.AddInstance.png)

After adding the instances, the Status will change to `Connected`, indicating that the internal network connection is successful. The IP CIDR shown earlier represents the internal IP addresses. For example, in the image below, the internal IPs of the two machines are 192.168.1.1 and 192.168.1.2, respectively.

![Connected](../../../../tutorials-images/07.DeepSeekSGLang/10.Connected.png)

#### Launch the service
After the model download is complete, run the command `ip -4 a`to check the internal network interface name and internal IP address, as shown in the image below:

- Internal IP: 192.168.1.1
- Internal network interface: meth340

![Internal Network](../../../../tutorials-images/07.DeepSeekSGLang/11.InternalNetwork.png)

Run the following commands on both nodes, one after the other:

```bash
# On Instance 1, run the following commands in order
# Note: Replace the values of GLOO_SOCKET_IFNAME and NCCL_SOCKET_IFNAME with the network interface name you found using the ip -4 a command.
# Note: Replace the values of SGLANG_HOST_IP and HOST_IP with the internal IP address of your designated master node.
export GLOO_SOCKET_IFNAME=meth340
export NCCL_SOCKET_IFNAME=meth340
export SGLANG_HOST_IP=192.168.1.1
export HOST_IP=192.168.1.1
python3 -m sglang.launch_server --model /home/DeepSeek-R1-Distill-Qwen-14B --port 8000 --tp 2 --dist-init-addr 192.168.1.1:6379 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0
```

![Instance1](../../../../tutorials-images/07.DeepSeekSGLang/12.Instance1.png)

```bash
# On Instance 2, run the following commands in order
# Note: Replace the values of GLOO_SOCKET_IFNAME and NCCL_SOCKET_IFNAME with the network interface name you found using the ip -4 a command.
# Note: Replace the values of SGLANG_HOST_IP and HOST_IP with the internal IP address of your designated master node.
export GLOO_SOCKET_IFNAME=meth341
export NCCL_SOCKET_IFNAME=meth341
export SGLANG_HOST_IP=192.168.1.1
export HOST_IP=192.168.1.1
python3 -m sglang.launch_server --model /home/DeepSeek-R1-Distill-Qwen-14B --port 8000 --tp 2 --dist-init-addr 192.168.1.1:6379 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0
```

![Instance2](../../../../tutorials-images/07.DeepSeekSGLang/13.Instance2.png)

The screenshot of normal operation is shown below. Model loading normally completes within 1 minute.

![Model Loading](../../../../tutorials-images/07.DeepSeekSGLang/14.ModelLoading.png)

## API Testing
On **My Instances** page, copy the HTTP Port 8000 link of the master node.
You can ignore the token part in the link. For example:

```bash
Copied link：https://tw-03.access.glows.ai:24516?token=hSFGABSM
Actual link：https://tw-03.access.glows.ai:24516
```

![HTTP Port 8000](../../../../tutorials-images/07.DeepSeekSGLang/15.HTTPPort8000.png)

The following is test code. Simply replace the base_url in the code with the link you copied.

```python
import openai
client = openai.Client(
    base_url="https://tw-03.access.glows.ai:24516/v1", api_key="EMPTY")

# Chat completion
response = client.chat.completions.create(
    model="default",
    messages=[
        {"role": "system", "content": "You are a helpful AI assistant"},
        {"role": "user", "content": "你是什么模型，谁开发的"},
    ],
    temperature=0.6
)
print(response)
```

![Demo](../../../../tutorials-images/07.DeepSeekSGLang/16.Demo.png)



## Data storage and snapshots
To use data storage and snapshot features, you first need to purchase storage space on the [Space Management](https://platform.glows.ai/space) page.

#### Data storage
During usage, if your program generates large output files saved on the machine’s hard drive, and you want to download them to your local device without keeping the instance running. You can move or copy the files to the `/datadrive` directory using `cp/mv` commands (or you can set the output path to `/datadrive` from the beginning in your program).
In this way, even after releasing the instance, you can still download the files from the [**Data Drive**](https://platform.glows.ai/data) page. Additionally, multiple instances can share the data stored in `/datadrive`.

For example, in this case where we are running a large model, normally we would need to download the model separately on each instance. However, if we download the model directly into `/datadrive` (or upload it there from local storage), there’s no need to download it again on each instance, and every instance created can access it immediately.

#### Snapshot
If you make any changes to the system environment during use — for example, installing new Python packages via pip — you can create a Snapshot afterward. Once you confirm that the environment is properly configured, go to the **My Instances** page and click the `Take Snapshot` button to create a one. You can then create a new instance directly from the snapshot without needing to reinstall or reconfigure the environment.

If you haven't made any changes to the environment, you don’t need to create a snapshot.

![Take Snapshot](../../../../tutorials-images/07.DeepSeekSGLang/17.TakeSnapshot.png)



## Shut down instances
After you finish using the machine and have saved the necessary environment or data, go to the **My Instances** page and click the `Release` button to shut down the instance.
Once the instance is released, billing will stop.
If you forget to release the instance, it will continue to incur charges. If your credits reach zero, the system will automatically create a snapshot and then release the machine.

If you have any questions or suggestions while using Glows.ai, feel free to contact us at any time.

